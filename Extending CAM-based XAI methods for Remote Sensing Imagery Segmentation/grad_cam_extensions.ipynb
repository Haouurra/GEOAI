{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Haouurra/GEOAI/blob/main/Extending%20CAM-based%20XAI%20methods%20for%20Remote%20Sensing%20Imagery%20Segmentation/grad_cam_extensions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch torchvision\n",
        "#!pip install segmentation-models-pytorch\n",
        "#!pip install rasterio\n",
        "#!pip install ttach==0.0.3\n",
        "!pip install grad-cam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkD52G1ZVyQf",
        "outputId": "e89b71d5-79f0-41d1-952e-0a735f235be2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.5.5.tar.gz (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from grad-cam) (11.2.1)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.21.0+cu124)\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.11/dist-packages (from grad-cam) (0.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from grad-cam) (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from grad-cam) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from grad-cam) (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.7.1->grad-cam) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.7.1->grad-cam) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->grad-cam) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->grad-cam) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->grad-cam) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.7.1->grad-cam) (3.0.2)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.5.5-py3-none-any.whl size=44284 sha256=75d860f8f77a91040db9b550752604867b1cf38b4e85d8ee11b3988c8b56f0b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/52/78/893c3b94279ef238f43a9e89608af648de401b96415bebbd1f\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: grad-cam\n",
            "Successfully installed grad-cam-1.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "ymLhorhNXVI1",
        "outputId": "cf83a80b-246e-468b-84e3-faed2feaff56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a7aba2b8-4884-4c43-99fc-8a7ba1b2b85a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a7aba2b8-4884-4c43-99fc-8a7ba1b2b85a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip GEOAI-ECRS2023.zip\n",
        "#!unzip pretrained_model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9K_GDX4XZnM",
        "outputId": "3f6ec254-3f5f-4361-e888-c9ddb520368d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open GEOAI-ECRS2023.zip, GEOAI-ECRS2023.zip.zip or GEOAI-ECRS2023.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd"
      ],
      "metadata": {
        "id": "pY0nQLxTXbA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRYGIE2PWxUn",
        "outputId": "1bbd34dd-e97a-488f-e2f3-c88cf7b920f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_2G1-KWV8Th",
        "outputId": "e826b741-794b-4fea-9fa5-6de5a7b8c897"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from segmentation_models_pytorch import Unet\n",
        "import torch\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model\n",
        "model = Unet(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "65ad4f3d0b0d447eb3ca6b0949607cad",
            "e6c1e8c98cc24ac5a72ab32d68382729",
            "a73b3088da4a4a8ab88aa9c3ee14c704",
            "4caa6d810e1849d5a43454cedf5708b3",
            "bc90a504b46e4c5e88e6a0f872e81447",
            "68142e32f3d14621abc72fba6203cecd",
            "78a8808712554975911c5ffdea9c5378",
            "0f4ea7c5a36248b5ba0b4d26e81d1823",
            "3636534f29614412af6c2bcd0fd92905",
            "d014f1c2a84643a089930c0d20699e2d",
            "0be65b8285a04fcda6d4f3ca72d893e3",
            "dcec344add5b4186bd8310dc02b86861",
            "e5a98b54833c4428b6c81fa2033eed39",
            "22dab3ccf07843848e8285def6dad267",
            "372fa7d07eca43b5b06b90432279dd2e",
            "ef9c00cba40b44dbba8cd0975e42c05e",
            "9a26e813fc714a8c9405836ae39e102d",
            "f93c43ff45ae44bdb96267a91bd045fd",
            "28fe152d928748cebbf510fc6b7a9fc5",
            "edb8eb1b563a4635afa306fd5a17c05b",
            "b085bbe5e4ee4a3184fb9fcafd0bb1e2",
            "3013d65f860f4da3baa221e984b9c9df"
          ]
        },
        "id": "iqEcXDxbV-Bi",
        "outputId": "428ce64f-4c63-4510-9d67-94538ca9415f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65ad4f3d0b0d447eb3ca6b0949607cad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/87.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcec344add5b4186bd8310dc02b86861"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XEhTVd2IsqWN"
      },
      "outputs": [],
      "source": [
        "# Import the required packages\n",
        "\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "from segmentation_models_pytorch import Unet\n",
        "\n",
        "from skimage.io import imread,imsave\n",
        "import numpy as np\n",
        "import rasterio as rio\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "\n",
        "from utils import *\n",
        "from color_map import cm_data\n",
        "from rasterio.features import shapes\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_WO8qJq3sqWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeae1c6d-f31a-404a-e353-1877b7934372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Unet(\n",
              "  (encoder): TimmUniversalEncoder(\n",
              "    (model): EfficientNetFeatures(\n",
              "      (conv_stem): Conv2dSame(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "      (bn1): BatchNormAct2d(\n",
              "        32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "        (drop): Identity()\n",
              "        (act): SiLU(inplace=True)\n",
              "      )\n",
              "      (blocks): Sequential(\n",
              "        (0): Sequential(\n",
              "          (0): DepthwiseSeparableConv(\n",
              "            (conv_dw): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pw): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              16, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(96, 96, kernel_size=(3, 3), stride=(2, 2), groups=96, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(144, 144, kernel_size=(5, 5), stride=(2, 2), groups=144, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              144, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              40, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(240, 240, kernel_size=(3, 3), stride=(2, 2), groups=240, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              240, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): InvertedResidual(\n",
              "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              480, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): InvertedResidual(\n",
              "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              112, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2dSame(672, 672, kernel_size=(5, 5), stride=(2, 2), groups=672, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              672, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (1): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (2): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "          (3): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "        (6): Sequential(\n",
              "          (0): InvertedResidual(\n",
              "            (conv_pw): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn1): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (conv_dw): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
              "            (bn2): BatchNormAct2d(\n",
              "              1152, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): SiLU(inplace=True)\n",
              "            )\n",
              "            (aa): Identity()\n",
              "            (se): SqueezeExcite(\n",
              "              (conv_reduce): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (act1): SiLU(inplace=True)\n",
              "              (conv_expand): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (gate): Sigmoid()\n",
              "            )\n",
              "            (conv_pwl): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (bn3): BatchNormAct2d(\n",
              "              320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True\n",
              "              (drop): Identity()\n",
              "              (act): Identity()\n",
              "            )\n",
              "            (drop_path): Identity()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): UnetDecoder(\n",
              "    (center): Identity()\n",
              "    (blocks): ModuleList(\n",
              "      (0): UnetDecoderBlock(\n",
              "        (conv1): Conv2dReLU(\n",
              "          (0): Conv2d(432, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention1): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "        (conv2): Conv2dReLU(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention2): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): UnetDecoderBlock(\n",
              "        (conv1): Conv2dReLU(\n",
              "          (0): Conv2d(296, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention1): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "        (conv2): Conv2dReLU(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention2): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "      )\n",
              "      (2): UnetDecoderBlock(\n",
              "        (conv1): Conv2dReLU(\n",
              "          (0): Conv2d(152, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention1): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "        (conv2): Conv2dReLU(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention2): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "      )\n",
              "      (3): UnetDecoderBlock(\n",
              "        (conv1): Conv2dReLU(\n",
              "          (0): Conv2d(80, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention1): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "        (conv2): Conv2dReLU(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention2): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "      )\n",
              "      (4): UnetDecoderBlock(\n",
              "        (conv1): Conv2dReLU(\n",
              "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention1): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "        (conv2): Conv2dReLU(\n",
              "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (attention2): Attention(\n",
              "          (attention): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (segmentation_head): SegmentationHead(\n",
              "    (0): Conv2d(16, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): Identity()\n",
              "    (2): Activation(\n",
              "      (activation): Sigmoid()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# load your model with pre-trained model weights\n",
        "\n",
        "MODEL_PATH = './pretrained_model'\n",
        "THRESH = 0.5\n",
        "ALPHA = 0.85\n",
        "SCALE = None\n",
        "\n",
        "\n",
        "model = Unet(\n",
        "        encoder_name = \"tu-tf_efficientnet_b0\",\n",
        "        encoder_depth= 5,\n",
        "        encoder_weights = None,\n",
        "        decoder_use_batchnorm = True,\n",
        "        decoder_channels = (256, 128, 64, 32, 16),\n",
        "        decoder_attention_type = None,\n",
        "        in_channels= 3,\n",
        "        classes = 3,\n",
        "        activation = 'sigmoid',\n",
        "        aux_params = None,\n",
        "    )\n",
        "\n",
        "model = load_model(model,MODEL_PATH)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "E-B-_IMNVT--"
      },
      "outputs": [],
      "source": [
        "# Define the CAM-based Extensions classes\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import ttach as tta\n",
        "import sys\n",
        "import torch\n",
        "import warnings\n",
        "from skimage.io import imread\n",
        "import matplotlib.pyplot as plt\n",
        "from segmentation_models_pytorch import Unet\n",
        "import argparse\n",
        "import os\n",
        "from typing import Callable, List\n",
        "import cv2\n",
        "import tqdm\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
        "from skimage.segmentation import watershed\n",
        "from skimage.measure import label\n",
        "from pytorch_grad_cam.utils.svd_on_activations import get_2d_projection\n",
        "from pytorch_grad_cam.utils.image import scale_cam_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.find_layers import replace_layer_recursive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "\n",
        "# Grad-Cam Classes\n",
        "class ActivationsAndGradients:\n",
        "    \"\"\" Class for extracting activations and\n",
        "    registering gradients from targetted intermediate layers \"\"\"\n",
        "\n",
        "    def __init__(self, model, target_layers, reshape_transform):\n",
        "        self.model = model\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.handles = []\n",
        "        for target_layer in target_layers:\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_activation))\n",
        "            # Because of https://github.com/pytorch/pytorch/issues/61519,\n",
        "            # we don't use backward hook to record gradients.\n",
        "            self.handles.append(\n",
        "                target_layer.register_forward_hook(self.save_gradient))\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        activation = output\n",
        "        if self.reshape_transform is not None:\n",
        "            activation = self.reshape_transform(activation)\n",
        "        self.activations.append(activation.cpu().detach())\n",
        "\n",
        "    def save_gradient(self, module, input, output):\n",
        "        if not hasattr(output, \"requires_grad\") or not output.requires_grad:\n",
        "            # You can only register hooks on tensor requires grad.\n",
        "            return\n",
        "\n",
        "        # Gradients are computed in reverse order\n",
        "        def _store_grad(grad):\n",
        "            if self.reshape_transform is not None:\n",
        "                grad = self.reshape_transform(grad)\n",
        "            self.gradients = [grad.cpu().detach()] + self.gradients\n",
        "            # self.gradients = [torch.mul(t, -1) for t in self.gradients]\n",
        "\n",
        "        output.register_hook(_store_grad)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        self.gradients = []\n",
        "        self.activations = []\n",
        "        return self.model(x)\n",
        "\n",
        "    def release(self):\n",
        "        for handle in self.handles:\n",
        "            handle.remove()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AblationLayer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AblationLayer, self).__init__()\n",
        "\n",
        "    def objectiveness_mask_from_svd(self, activations, threshold=0.01):\n",
        "        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\n",
        "            The idea is to apply the EigenCAM method by doing PCA on the activations.\n",
        "            Then we create a binary mask by comparing to a low threshold.\n",
        "            Areas that are masked out, are probably not interesting anyway.\n",
        "        \"\"\"\n",
        "\n",
        "        projection = get_2d_projection(activations[None, :])[0, :]\n",
        "        projection = np.abs(projection)\n",
        "        projection = projection - projection.min()\n",
        "        projection = projection / projection.max()\n",
        "        projection = projection > threshold\n",
        "        return projection\n",
        "\n",
        "    def activations_to_be_ablated(\n",
        "            self,\n",
        "            activations,\n",
        "            ratio_channels_to_ablate=1.0):\n",
        "        \"\"\" Experimental method to get a binary mask to compare if the activation is worth ablating.\n",
        "            Create a binary CAM mask with objectiveness_mask_from_svd.\n",
        "            Score each Activation channel, by seeing how much of its values are inside the mask.\n",
        "            Then keep the top channels.\n",
        "\n",
        "        \"\"\"\n",
        "        if ratio_channels_to_ablate == 1.0:\n",
        "            self.indices = np.int32(range(activations.shape[0]))\n",
        "            return self.indices\n",
        "\n",
        "        projection = self.objectiveness_mask_from_svd(activations)\n",
        "\n",
        "        scores = []\n",
        "        for channel in activations:\n",
        "            normalized = np.abs(channel)\n",
        "            normalized = normalized - normalized.min()\n",
        "            normalized = normalized / np.max(normalized)\n",
        "            score = (projection * normalized).sum() / normalized.sum()\n",
        "            scores.append(score)\n",
        "        scores = np.float32(scores)\n",
        "\n",
        "        indices = list(np.argsort(scores))\n",
        "        high_score_indices = indices[::-\n",
        "                                     1][: int(len(indices) *\n",
        "                                              ratio_channels_to_ablate)]\n",
        "        low_score_indices = indices[: int(\n",
        "            len(indices) * ratio_channels_to_ablate)]\n",
        "        self.indices = np.int32(high_score_indices + low_score_indices)\n",
        "        return self.indices\n",
        "\n",
        "    def set_next_batch(\n",
        "            self,\n",
        "            input_batch_index,\n",
        "            activations,\n",
        "            num_channels_to_ablate):\n",
        "        \"\"\" This creates the next batch of activations from the layer.\n",
        "            Just take corresponding batch member from activations, and repeat it num_channels_to_ablate times.\n",
        "        \"\"\"\n",
        "        self.activations = activations[input_batch_index, :, :, :].clone(\n",
        "        ).unsqueeze(0).repeat(num_channels_to_ablate, 1, 1, 1)\n",
        "\n",
        "    def __call__(self, x, test=None):\n",
        "        output = self.activations\n",
        "        for i in range(output.size(0)):\n",
        "            # Commonly the minimum activation will be 0,\n",
        "            # And then it makes sense to zero it out.\n",
        "            # However depending on the architecture,\n",
        "            # If the values can be negative, we use very negative values\n",
        "            # to perform the ablation, deviating from the paper.\n",
        "            if torch.min(output) == 0:\n",
        "                output[i, self.indices[i], :] = 0\n",
        "            else:\n",
        "                ABLATION_VALUE = 1e7\n",
        "                output[i, self.indices[i], :] = torch.min(\n",
        "                    output) - ABLATION_VALUE\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GRADCAM_Extensions:\n",
        "    def __init__(self, extension, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None, compute_input_gradient: bool = False,\n",
        "                 uses_gradients: bool = True) -> None:\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
        "        self.extension = extension\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n",
        "        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
        "        target_size = input_tensor.size(-1), input_tensor.size(-2)\n",
        "        # print('Activations list size: ', len(activations_list))\n",
        "        # print('Gradients list size: ', len(grads_list))\n",
        "        # print('Target layer size: ', len(self.target_layers))\n",
        "\n",
        "        cam_per_target_layer = []\n",
        "        # Loop over the saliency image from every layer\n",
        "        for i in range(len(self.target_layers)):\n",
        "            target_layer = self.target_layers[i]\n",
        "            # print('\\t\\t\\t-----------------------\\n')\n",
        "            # print('Target Layer ', i + 1, ': ', target_layer)\n",
        "            layer_activations = None\n",
        "            layer_grads = None\n",
        "            if i < len(activations_list):\n",
        "                layer_activations = activations_list[i]\n",
        "            if i < len(grads_list):\n",
        "                layer_grads = grads_list[i]\n",
        "\n",
        "\n",
        "            if  self.extension == \"grad_cam\":\n",
        "                weights = np.mean(layer_grads, axis=(2, 3))\n",
        "                weighs_up = weights[:, :, None, None]\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"hires_cam\":\n",
        "                elementwise_activations = layer_grads * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(elementwise_activations)\n",
        "                else:\n",
        "                    cam = elementwise_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"ew_cam\":\n",
        "                elementwise_activations = np.maximum(layer_grads * layer_activations, 0)\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(elementwise_activations)\n",
        "                else:\n",
        "                    cam = elementwise_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"grad_cam_pp\":\n",
        "                grads_power_2 = layer_grads**2\n",
        "                grads_power_3 = grads_power_2 * layer_grads\n",
        "                # Equation 19 in https://arxiv.org/abs/1710.11063\n",
        "                sum_activations = np.sum(layer_activations, axis=(2, 3))\n",
        "                eps = 0.000001\n",
        "                aij = grads_power_2 / (2 * grads_power_2 + sum_activations[:, :, None, None] * grads_power_3 + eps)\n",
        "                # Now bring back the ReLU from eq.7 in the paper,\n",
        "                # And zero out aijs where the activations are 0\n",
        "                aij = np.where(layer_grads != 0, aij, 0)\n",
        "                weights = np.maximum(layer_grads, 0) * aij\n",
        "                weights = np.sum(weights, axis=(2, 3))\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "\n",
        "\n",
        "            elif self.extension == \"x_grad_cam\":\n",
        "                sum_activations = np.sum(layer_activations, axis=(2, 3))\n",
        "                eps = 1e-7\n",
        "                weights = layer_grads * layer_activations / \\\n",
        "                (sum_activations[:, :, None, None] + eps)\n",
        "                weights = weights.sum(axis=(2, 3))\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "\n",
        "            elif self.extension == \"score_cam\":\n",
        "                with torch.no_grad():\n",
        "                    upsample = torch.nn.UpsamplingBilinear2d(size=input_tensor.shape[-2:])\n",
        "                    activation_tensor = torch.from_numpy(layer_activations)\n",
        "                    if self.cuda:\n",
        "                        activation_tensor = activation_tensor.cuda()\n",
        "                    upsampled = upsample(activation_tensor)\n",
        "                    maxs = upsampled.view(upsampled.size(0), upsampled.size(1), -1).max(dim=-1)[0]\n",
        "                    mins = upsampled.view(upsampled.size(0), upsampled.size(1), -1).min(dim=-1)[0]\n",
        "                    maxs, mins = maxs[:, :, None, None], mins[:, :, None, None]\n",
        "                    upsampled = (upsampled - mins) / (maxs - mins)\n",
        "\n",
        "                    input_tensors = input_tensor[:, None, :, :] * upsampled[:, :, None, :, :]\n",
        "                    if hasattr(self, \"batch_size\"):\n",
        "                        BATCH_SIZE = self.batch_size\n",
        "                    else:\n",
        "                        BATCH_SIZE = 8\n",
        "\n",
        "\n",
        "                    scores = []\n",
        "                    for target, tensor in zip(targets, input_tensors):\n",
        "                        for i in tqdm.tqdm(range(0, tensor.size(0), BATCH_SIZE)):\n",
        "                            batch = tensor[i: i + BATCH_SIZE, :]\n",
        "                            outputs = [target(o).cpu().item() for o in self.model(batch)]\n",
        "                            scores.extend(outputs)\n",
        "                    scores = torch.Tensor(scores)\n",
        "                    scores = scores.view(layer_activations.shape[0], layer_activations.shape[1])\n",
        "                    weights = torch.nn.Softmax(dim=-1)(scores).numpy()\n",
        "\n",
        "                weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(weighted_activations)\n",
        "                    print(\"Cam image per layer size: \", cam.shape)\n",
        "                else:\n",
        "                    cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "            elif self.extension == \"layer_cam\":\n",
        "                spatial_weighted_activations = np.maximum(layer_grads, 0) * layer_activations\n",
        "                if eigen_smooth:\n",
        "                    cam = get_2d_projection(spatial_weighted_activations)\n",
        "                else:\n",
        "                    cam = spatial_weighted_activations.sum(axis=1)\n",
        "\n",
        "\n",
        "            elif self.extension == \"eigen_cam\":\n",
        "                cam = get_2d_projection(layer_activations)\n",
        "\n",
        "            elif self.extension == \"eigen_grad_cam\":\n",
        "                cam = get_2d_projection(layer_grads * layer_activations)\n",
        "\n",
        "            else:\n",
        "                print(\"Unkown Extension. Please use one of the following: grad_cam - hires_cam - ew_cam -  grad_cam_pp - x_grad_cam - score_cam - layer_cam - eigen_cam -  eigen_grad_cam\")\n",
        "\n",
        "\n",
        "\n",
        "            cam = np.maximum(cam, 0)\n",
        "            # print(\"Cam image  Max per layer size: \", cam.shape)\n",
        "            scaled = scale_cam_image(cam, target_size)\n",
        "            # print(\"Scaled Cam image per layer size: \", scaled.shape)\n",
        "            cam_per_target_layer.append(scaled[:, None, :])\n",
        "\n",
        "        # print(\"Cam image list size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        # print(\"Cam image list Concat size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
        "        # print(\"Cam image list (max) size: \", len(cam_per_target_layer))\n",
        "        result = np.mean(cam_per_target_layer, axis=1) # old: mean\n",
        "        # print(\"+++ Averaged CAM list size: \", result.shape)\n",
        "\n",
        "\n",
        "        return scale_cam_image(result) # result\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            transforms = tta.Compose(\n",
        "                [\n",
        "                    tta.HorizontalFlip(),\n",
        "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "                ]\n",
        "            )\n",
        "            cams = []\n",
        "            for transform in transforms:\n",
        "                augmented_tensor = transform.augment_image(input_tensor)\n",
        "                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n",
        "                # The ttach library expects a tensor of size BxCxHxW\n",
        "                cam = cam[:, None, :, :]\n",
        "                cam = torch.from_numpy(cam)\n",
        "                cam = transform.deaugment_mask(cam)\n",
        "                # Back to numpy float32, HxW\n",
        "                cam = cam.numpy()\n",
        "                cam = cam[:, 0, :, :]\n",
        "                cams.append(cam)\n",
        "            cam = np.mean(np.float32(cams), axis=0)\n",
        "            return cam\n",
        "        else:\n",
        "            return self.forward(input_tensor, targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class GRADCAMEXTENDED_AblationCAM:\n",
        "    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None, compute_input_gradient: bool = False,\n",
        "                 uses_gradients: bool = True,\n",
        "                 ablation_layer: torch.nn.Module = AblationLayer(),\n",
        "                 batch_size: int = 32,\n",
        "                 ratio_channels_to_ablate: float = 1.0) -> None:\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
        "        self.batch_size = batch_size\n",
        "        self.ablation_layer = ablation_layer\n",
        "        self.ratio_channels_to_ablate = ratio_channels_to_ablate\n",
        "\n",
        "\n",
        "    def save_activation(self, module, input, output) -> None:\n",
        "        \"\"\" Helper function to save the raw activations from the target layer \"\"\"\n",
        "        self.activations = output\n",
        "\n",
        "    def assemble_ablation_scores(self,\n",
        "                                 new_scores: list,\n",
        "                                 original_score: float,\n",
        "                                 ablated_channels: np.ndarray,\n",
        "                                 number_of_channels: int) -> np.ndarray:\n",
        "        \"\"\" Take the value from the channels that were ablated,\n",
        "            and just set the original score for the channels that were skipped \"\"\"\n",
        "\n",
        "        index = 0\n",
        "        result = []\n",
        "        sorted_indices = np.argsort(ablated_channels)\n",
        "        ablated_channels = ablated_channels[sorted_indices]\n",
        "        new_scores = np.float32(new_scores)[sorted_indices]\n",
        "\n",
        "        for i in range(number_of_channels):\n",
        "            if index < len(ablated_channels) and ablated_channels[index] == i:\n",
        "                weight = new_scores[index]\n",
        "                index = index + 1\n",
        "            else:\n",
        "                weight = original_score\n",
        "            result.append(weight)\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        activations_list = [a.cpu().data.numpy() for a in self.activations_and_grads.activations]\n",
        "        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
        "        target_size = input_tensor.size(-1), input_tensor.size(-2)\n",
        "        # print('Activations list size: ', len(activations_list))\n",
        "        # print('Gradients list size: ', len(grads_list))\n",
        "        # print('Target layer size: ', len(self.target_layers))\n",
        "\n",
        "        cam_per_target_layer = []\n",
        "        # Loop over the saliency image from every layer\n",
        "        for i in range(len(self.target_layers)):\n",
        "            target_layer = self.target_layers[i]\n",
        "            # print('\\t\\t\\t-----------------------\\n')\n",
        "            # print('Target Layer ', i + 1, ': ', target_layer)\n",
        "            layer_activations = None\n",
        "            layer_grads = None\n",
        "            if i < len(activations_list):\n",
        "                layer_activations = activations_list[i]\n",
        "            if i < len(grads_list):\n",
        "                layer_grads = grads_list[i]\n",
        "\n",
        "\n",
        "            # get weights\n",
        "            # Do a forward pass, compute the target scores, and cache the\n",
        "            # activations\n",
        "            handle = target_layer.register_forward_hook(self.save_activation)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_tensor)\n",
        "                handle.remove()\n",
        "                original_scores = np.float32(\n",
        "                    [target(output).cpu().item() for target, output in zip(targets, outputs)])\n",
        "\n",
        "            # Replace the layer with the ablation layer.\n",
        "            # When we finish, we will replace it back, so the original model is\n",
        "            # unchanged.\n",
        "            ablation_layer = self.ablation_layer\n",
        "            replace_layer_recursive(self.model, target_layer, ablation_layer)\n",
        "\n",
        "            number_of_channels = layer_activations.shape[1]\n",
        "            weights = []\n",
        "            # This is a \"gradient free\" method, so we don't need gradients here.\n",
        "            with torch.no_grad():\n",
        "                # Loop over each of the batch images and ablate activations for it.\n",
        "                for batch_index, (target, tensor) in enumerate(\n",
        "                        zip(targets, input_tensor)):\n",
        "                    new_scores = []\n",
        "                    batch_tensor = tensor.repeat(self.batch_size, 1, 1, 1)\n",
        "\n",
        "                    # Check which channels should be ablated. Normally this will be all channels,\n",
        "                    # But we can also try to speed this up by using a low\n",
        "                    # ratio_channels_to_ablate.\n",
        "                    channels_to_ablate = ablation_layer.activations_to_be_ablated(\n",
        "                        layer_activations[batch_index, :], self.ratio_channels_to_ablate)\n",
        "                    number_channels_to_ablate = len(channels_to_ablate)\n",
        "\n",
        "                    for i in tqdm.tqdm(range(0, number_channels_to_ablate, self.batch_size)):\n",
        "                        if i + self.batch_size > number_channels_to_ablate:\n",
        "                            batch_tensor = batch_tensor[:(number_channels_to_ablate - i)]\n",
        "\n",
        "                        # Change the state of the ablation layer so it ablates the next channels.\n",
        "                        # TBD: Move this into the ablation layer forward pass.\n",
        "                        ablation_layer.set_next_batch(input_batch_index=batch_index,activations=self.activations,\n",
        "                            num_channels_to_ablate=batch_tensor.size(0))\n",
        "\n",
        "\n",
        "                        score = [target(o).cpu().item() for o in self.model(batch_tensor)]\n",
        "                        new_scores.extend(score)\n",
        "\n",
        "                        ablation_layer.indices = ablation_layer.indices[batch_tensor.size(0):]\n",
        "\n",
        "\n",
        "                    new_scores = self.assemble_ablation_scores(new_scores,original_scores[batch_index], channels_to_ablate,\n",
        "                        number_of_channels)\n",
        "                    weights.extend(new_scores)\n",
        "\n",
        "            weights = np.float32(weights)\n",
        "            weights = weights.reshape(layer_activations.shape[:2])\n",
        "            original_scores = original_scores[:, None]\n",
        "            weights = (original_scores - weights) / original_scores\n",
        "\n",
        "            # Replace the model back to the original state\n",
        "            #-----------------------------------\n",
        "            replace_layer_recursive(self.model, ablation_layer, target_layer)\n",
        "\n",
        "\n",
        "            # Equation 3.1\n",
        "            weighted_activations = weights[:, :, None, None] * layer_activations\n",
        "\n",
        "            # Equation 3.2\n",
        "            if eigen_smooth:\n",
        "                cam = get_2d_projection(weighted_activations)\n",
        "            else:\n",
        "                cam = weighted_activations.sum(axis=1)\n",
        "\n",
        "            cam = np.maximum(cam, 0)\n",
        "            # print(\"Cam image  Max per layer size: \", cam.shape)\n",
        "            scaled = scale_cam_image(cam, target_size)\n",
        "            # print(\"Scaled Cam image per layer size: \", scaled.shape)\n",
        "            cam_per_target_layer.append(scaled[:, None, :])\n",
        "\n",
        "        # print(\"Cam image list size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        # print(\"Cam image list Concat size: \", len(cam_per_target_layer))\n",
        "        cam_per_target_layer = np.maximum(cam_per_target_layer, 0)\n",
        "        # print(\"Cam image list (max) size: \", len(cam_per_target_layer))\n",
        "        result = np.mean(cam_per_target_layer, axis=1) # old: mean\n",
        "        # print(\"+++ Averaged CAM list size: \", result.shape)\n",
        "\n",
        "\n",
        "        return scale_cam_image(result) # result\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            transforms = tta.Compose(\n",
        "                [\n",
        "                    tta.HorizontalFlip(),\n",
        "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "                ]\n",
        "            )\n",
        "            cams = []\n",
        "            for transform in transforms:\n",
        "                augmented_tensor = transform.augment_image(input_tensor)\n",
        "                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n",
        "                # The ttach library expects a tensor of size BxCxHxW\n",
        "                cam = cam[:, None, :, :]\n",
        "                cam = torch.from_numpy(cam)\n",
        "                cam = transform.deaugment_mask(cam)\n",
        "                # Back to numpy float32, HxW\n",
        "                cam = cam.numpy()\n",
        "                cam = cam[:, 0, :, :]\n",
        "                cams.append(cam)\n",
        "            cam = np.mean(np.float32(cams), axis=0)\n",
        "            return cam\n",
        "        else:\n",
        "            return self.forward(input_tensor, targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pytorch_grad_cam.utils.find_layers import find_layer_predicate_recursive\n",
        "from pytorch_grad_cam.utils.image import scale_accross_batch_and_channels\n",
        "\n",
        "\n",
        "class GRADCAMEXTENDED_FullGrad:\n",
        "    def __init__(self, model: torch.nn.Module, target_layers: List[torch.nn.Module], use_cuda: bool = False,\n",
        "                 reshape_transform: Callable = None, compute_input_gradient: bool = True,\n",
        "                 uses_gradients: bool = True) -> None:\n",
        "\n",
        "        if len(target_layers) > 0:\n",
        "            print(\n",
        "                \"Warning: target_layers is ignored in FullGrad. All bias layers will be used instead\")\n",
        "\n",
        "        def layer_with_2D_bias(layer):\n",
        "            bias_target_layers = [torch.nn.Conv2d, torch.nn.BatchNorm2d]\n",
        "            if type(layer) in bias_target_layers and layer.bias is not None:\n",
        "                return True\n",
        "            return False\n",
        "        target_layers = find_layer_predicate_recursive(model, layer_with_2D_bias)\n",
        "        self.bias_data = [self.get_bias_data(layer).cpu().numpy() for layer in target_layers]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.model = model.eval()\n",
        "        self.target_layers = target_layers\n",
        "        self.cuda = use_cuda\n",
        "        if self.cuda:\n",
        "            self.model = model.cuda()\n",
        "        self.reshape_transform = reshape_transform\n",
        "        self.compute_input_gradient = compute_input_gradient\n",
        "        self.uses_gradients = uses_gradients\n",
        "        self.activations_and_grads = ActivationsAndGradients(self.model, target_layers, reshape_transform)\n",
        "\n",
        "\n",
        "    def get_bias_data(self, layer):\n",
        "        # Borrowed from official paper impl:\n",
        "        # https://github.com/idiap/fullgrad-saliency/blob/master/saliency/tensor_extractor.py#L47\n",
        "        if isinstance(layer, torch.nn.BatchNorm2d):\n",
        "            bias = - (layer.running_mean * layer.weight\n",
        "                      / torch.sqrt(layer.running_var + layer.eps)) + layer.bias\n",
        "            return bias.data\n",
        "        else:\n",
        "            return layer.bias.data\n",
        "\n",
        "    def forward(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module],\n",
        "                eigen_smooth: bool = False) -> np.ndarray:\n",
        "\n",
        "        # print('\\n----------------------------------------Seg-Grad-Cam----------------------------------------------------\\n')\n",
        "\n",
        "        if self.cuda:\n",
        "            input_tensor = input_tensor.cuda()\n",
        "        if self.compute_input_gradient:\n",
        "            input_tensor = torch.autograd.Variable(input_tensor, requires_grad=True)\n",
        "\n",
        "        outputs = self.activations_and_grads(input_tensor)\n",
        "        if targets is None:\n",
        "            target_categories = np.argmax(outputs.cpu().data.numpy(), axis=-1)\n",
        "            targets = [ClassifierOutputTarget(category) for category in target_categories]\n",
        "\n",
        "        if self.uses_gradients:\n",
        "            self.model.zero_grad()\n",
        "            loss = sum([target(output) for target, output in zip(targets, outputs)])\n",
        "            loss.backward(retain_graph=True)\n",
        "\n",
        "        #----remove------\n",
        "\n",
        "        input_grad = input_tensor.grad.data.cpu().numpy()\n",
        "        grads_list = [g.cpu().data.numpy() for g in self.activations_and_grads.gradients]\n",
        "        cam_per_target_layer = []\n",
        "        target_size = input_tensor.size(-1), input_tensor.size(-2)\n",
        "\n",
        "        gradient_multiplied_input = input_grad * input_tensor.data.cpu().numpy()\n",
        "        gradient_multiplied_input = np.abs(gradient_multiplied_input)\n",
        "        gradient_multiplied_input = scale_accross_batch_and_channels(\n",
        "            gradient_multiplied_input,\n",
        "            target_size)\n",
        "        cam_per_target_layer.append(gradient_multiplied_input)\n",
        "\n",
        "        # Loop over the saliency image from every layer\n",
        "        assert(len(self.bias_data) == len(grads_list))\n",
        "        for bias, grads in zip(self.bias_data, grads_list):\n",
        "            bias = bias[None, :, None, None]\n",
        "            # In the paper they take the absolute value,\n",
        "            # but possibily taking only the positive gradients will work\n",
        "            # better.\n",
        "            bias_grad = np.abs(bias * grads)\n",
        "            result = scale_accross_batch_and_channels(\n",
        "                bias_grad, target_size)\n",
        "            result = np.sum(result, axis=1)\n",
        "            cam_per_target_layer.append(result[:, None, :])\n",
        "        cam_per_target_layer = np.concatenate(cam_per_target_layer, axis=1)\n",
        "        if eigen_smooth:\n",
        "            # Resize to a smaller image, since this method typically has a very large number of channels,\n",
        "            # and then consumes a lot of memory\n",
        "            cam_per_target_layer = scale_accross_batch_and_channels(\n",
        "                cam_per_target_layer, (target_size[0] // 8, target_size[1] // 8))\n",
        "            cam_per_target_layer = get_2d_projection(cam_per_target_layer)\n",
        "            cam_per_target_layer = cam_per_target_layer[:, None, :, :]\n",
        "            cam_per_target_layer = scale_accross_batch_and_channels(\n",
        "                cam_per_target_layer,\n",
        "                target_size)\n",
        "        else:\n",
        "            cam_per_target_layer = np.sum(\n",
        "                cam_per_target_layer, axis=1)[:, None, :]\n",
        "\n",
        "\n",
        "        result = np.sum(cam_per_target_layer, axis=1)\n",
        "        return scale_cam_image(result) # result\n",
        "\n",
        "    def __call__(self, input_tensor: torch.Tensor, targets: List[torch.nn.Module] = None, aug_smooth: bool = False,\n",
        "                 eigen_smooth: bool = False) -> np.ndarray:\n",
        "        # Smooth the CAM result with test time augmentation\n",
        "        if aug_smooth is True:\n",
        "            transforms = tta.Compose(\n",
        "                [\n",
        "                    tta.HorizontalFlip(),\n",
        "                    tta.Multiply(factors=[0.9, 1, 1.1]),\n",
        "                ]\n",
        "            )\n",
        "            cams = []\n",
        "            for transform in transforms:\n",
        "                augmented_tensor = transform.augment_image(input_tensor)\n",
        "                cam = self.forward(augmented_tensor, targets, eigen_smooth)\n",
        "                # The ttach library expects a tensor of size BxCxHxW\n",
        "                cam = cam[:, None, :, :]\n",
        "                cam = torch.from_numpy(cam)\n",
        "                cam = transform.deaugment_mask(cam)\n",
        "                # Back to numpy float32, HxW\n",
        "                cam = cam.numpy()\n",
        "                cam = cam[:, 0, :, :]\n",
        "                cams.append(cam)\n",
        "            cam = np.mean(np.float32(cams), axis=0)\n",
        "            return cam\n",
        "        else:\n",
        "            return self.forward(input_tensor, targets, eigen_smooth)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.activations_and_grads.release()\n",
        "\n",
        "    def __enter__(self):\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, exc_tb):\n",
        "        self.activations_and_grads.release()\n",
        "        if isinstance(exc_value, IndexError):\n",
        "            # Handle IndexError here...\n",
        "            print(\n",
        "                f\"An exception occurred in CAM with block: {exc_type}. Message: {exc_value}\")\n",
        "            return True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the original segmentation score\n",
        "original_pred_mask = get_segmentation_mask(model, full_img, rrp_info)\n",
        "original_score = evaluate_segmentation(original_pred_mask, full_img_gt)\n",
        "\n",
        "# M3: Highlighted + GT (masking the important region)\n",
        "union_mask = np.logical_or(im_bw, full_img_gt)  # binary mask\n",
        "masked_img = full_img * np.logical_not(union_mask)[..., None]  # occlude union region\n",
        "\n",
        "# Get prediction on masked image\n",
        "masked_pred_mask = get_segmentation_mask(model, masked_img, rrp_info)\n",
        "masked_score = evaluate_segmentation(masked_pred_mask, full_img_gt)\n",
        "\n",
        "# Calculate drop in segmentation score\n",
        "drop_score = original_score - masked_score\n",
        "\n",
        "# Store results\n",
        "results['threshold'].append(thres)\n",
        "results['method'].append('Grad-CAM')\n",
        "results['mode'].append('M3-drop')\n",
        "results['confidence'].append(drop_score)\n",
        "results['entropy'].append(np.nan)  # Not needed now, or keep old entropy if you want\n"
      ],
      "metadata": {
        "id": "5DNX7IqNlYck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_segmentation_mask(model, img, rrp_info):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        input_tensor = preprocess(img)  # e.g., normalization, resizing\n",
        "        input_tensor = input_tensor.to(device).unsqueeze(0)\n",
        "        output = model(input_tensor)[0]\n",
        "        pred_mask = torch.argmax(output, dim=0).cpu().numpy()\n",
        "    return pred_mask\n",
        "\n",
        "def evaluate_segmentation(pred_mask, gt_mask):\n",
        "    intersection = np.logical_and(pred_mask == 1, gt_mask == 1).sum()\n",
        "    union = np.logical_or(pred_mask == 1, gt_mask == 1).sum()\n",
        "    iou = intersection / union if union != 0 else 0\n",
        "    return iou  # or Dice, etc.\n"
      ],
      "metadata": {
        "id": "SIvJSjC1lY0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "editable": true,
        "id": "8qvw_iUTsqWh",
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "041c1c67-6899-47e1-f419-47679578094a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: b'./dataset/images'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f7fc7c8c1638>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_gt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_gt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'./dataset/images'"
          ]
        }
      ],
      "source": [
        "# Apply the adapted CAM-based Extensions on the considered dataset\n",
        "\n",
        "class SemanticSegmentationTarget:\n",
        "    def __init__(self, category, mask):\n",
        "        self.category = category\n",
        "        self.mask = torch.from_numpy(mask)\n",
        "        if torch.cuda.is_available():\n",
        "            self.mask = self.mask.cuda()\n",
        "\n",
        "    def __call__(self, model_output):\n",
        "        return (model_output[self.category, :, : ] * self.mask).sum()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def defaultScales():\n",
        "    classes_cmap = plt.get_cmap('Spectral', 20)\n",
        "    scale_fig = 2\n",
        "    fonts = 15\n",
        "    scatter_size = 330 * scale_fig\n",
        "    return classes_cmap, scale_fig, fonts, scatter_size\n",
        "\n",
        "\n",
        "def show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight):\n",
        "    heatmap_sgc = cv2.applyColorMap(np.uint8(255 * grayscale_cam_EX), cv2.COLORMAP_JET)\n",
        "    heatmap_sgc = cv2.cvtColor(heatmap_sgc, cv2.COLOR_BGR2RGB)\n",
        "    heatmap_sgc = np.float32(heatmap_sgc) / 255\n",
        "    if full_img.shape[-1]==4:\n",
        "        heatmap_sgc = cv2.cvtColor(heatmap_sgc,cv2.COLOR_RGB2RGBA)\n",
        "    Exmap_sgc = (1 - image_weight) * heatmap_sgc + image_weight * full_img_rgba\n",
        "    Exmap_sgc = Exmap_sgc / np.max(Exmap_sgc)\n",
        "    Exmap_sgc = np.uint8(255 * Exmap_sgc)\n",
        "\n",
        "    return Exmap_sgc, heatmap_sgc\n",
        "\n",
        "\n",
        "def prob_2_entropy(prob):\n",
        "    \"\"\" convert probabilistic prediction maps to weighted self-information maps\n",
        "    \"\"\"\n",
        "    n, c, h, w = prob.size()\n",
        "    return -torch.mul(prob, torch.log2(prob + 1e-30)) / np.log2(c)\n",
        "\n",
        "def XAI_EVAL_M2(thres,grayscale_cam_EX, full_img_gt, full_img, model, rrp_info,target_category):\n",
        "    im_bw_sgc = cv2.threshold(grayscale_cam_EX, thres, 1, cv2.THRESH_BINARY)[1]\n",
        "    union_gt_sgc = np.ma.mask_or(full_img_gt,im_bw_sgc)\n",
        "\n",
        "    E_sgc = full_img * union_gt_sgc[..., None]\n",
        "    x_sgc = totensor(E_sgc)\n",
        "    x_sgc = x_sgc.cuda()\n",
        "    with torch.no_grad():\n",
        "        y_pred_sgc = model(x_sgc)\n",
        "        y_pred_sgc = unpad_resize(y_pred_sgc,rrp_info)\n",
        "\n",
        "    mask_tensor_sgc = y_pred_sgc[0,...]\n",
        "    mask_sgc = y_pred_sgc[0,...].cpu().numpy().transpose(1,2,0)\n",
        "    target_mask_f = np.float32(mask_sgc[:,:,target_category]) * full_img_gt\n",
        "    target_Confidence_score = target_mask_f[np.nonzero(target_mask_f)]\n",
        "    target_Confidence_score_sgc = np.mean(target_Confidence_score)\n",
        "\n",
        "    logist_softmax_entropy_sgc = prob_2_entropy(y_pred_sgc)\n",
        "    target_entropy_mask_sgc = logist_softmax_entropy_sgc[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n",
        "    target_entropy_mask_class_sgc = target_entropy_mask_sgc[:,:,target_category] * full_img_gt\n",
        "    target_entropy_mask_class_sgc_sc = (np.mean(target_entropy_mask_class_sgc))\n",
        "    return target_Confidence_score_sgc, target_entropy_mask_class_sgc_sc, im_bw_sgc,E_sgc\n",
        "\n",
        "\n",
        "\n",
        "images_dir = r'./dataset/images'\n",
        "gt_dir = r'./dataset/gt'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ticks = np.linspace(0, 1, 6, endpoint=True)\n",
        "classes_cmap, scale_fig, fonts, scatter_size = defaultScales()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "decoder_idx = 1\n",
        "n_xai = 6\n",
        "n_imgs = 1\n",
        "target_layers =  [model.decoder.blocks[decoder_idx - 1]]\n",
        "target_category = 0\n",
        "XAI_method = [\"grad_cam\", \"hires_cam\", \"ew_cam\", \"grad_cam_pp\", \"x_grad_cam\",\"score_cam\", \"layer_cam\", \"eigen_cam\", \"eigen_grad_cam\"]\n",
        "\n",
        "image_weight = 0.006\n",
        "\n",
        "directory_images = os.fsencode(images_dir)\n",
        "directory_gt = os.fsencode(gt_dir)\n",
        "\n",
        "\n",
        "number_testing_images = 0\n",
        "\n",
        "Model_Seg_Score = np.zeros((n_imgs))\n",
        "Model_Seg_Entropy = np.zeros((n_imgs))\n",
        "\n",
        "Seg_Score = np.zeros((n_xai, n_imgs))\n",
        "Seg_Entropy = np.zeros((n_xai, n_imgs))\n",
        "counter = 0\n",
        "\n",
        "\n",
        "for (file_img, file_gt) in zip(os.listdir(directory_images), os.listdir(directory_gt) ):\n",
        "\n",
        "    print(file_img, file_gt)\n",
        "    if file_img == file_gt:\n",
        "        number_testing_images = number_testing_images + 1\n",
        "        print(\"Testing Image: \", number_testing_images)\n",
        "        filename = os.fsdecode(file_img)\n",
        "        filename_gt = os.fsdecode(file_gt)\n",
        "        if filename.endswith(\".png\") and filename_gt.endswith(\".png\"):\n",
        "            raster_file = rio.open(f'{images_dir}/{filename}')\n",
        "            full_img = raster_file.read().transpose(1,2,0)\n",
        "            full_img,rrp_info = ratio_resize_pad(full_img, ratio = None)\n",
        "            full_img_rgba = full_img\n",
        "            print(\"Input Image Shape: \", full_img.shape)\n",
        "            if full_img.shape[-1]==4:full_img = cv2.cvtColor(full_img,cv2.COLOR_RGBA2RGB) #WHU images are RGBA\n",
        "            # read gt mask\n",
        "            raster_file_gt = rio.open(f'{gt_dir}/{filename_gt}')\n",
        "            full_img_gt = raster_file_gt.read().transpose(1,2,0)\n",
        "            full_img_gt,rrp_info_gt = ratio_resize_pad(full_img_gt, ratio = None)\n",
        "            full_img_gt = np.float32(full_img_gt) / np.max(full_img_gt)\n",
        "            print(\"GT mask shape: \", full_img_gt.shape)\n",
        "\n",
        "\n",
        "            full_img = normalize(full_img)\n",
        "            x = totensor(full_img)\n",
        "            x = x.cuda()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                y_pred = model(x)\n",
        "                y_pred = unpad_resize(y_pred,rrp_info)\n",
        "\n",
        "\n",
        "\n",
        "            mask_tensor = y_pred[0,...]\n",
        "            print('predicated tensor shape: ', mask_tensor.shape)\n",
        "            mask = y_pred[0,...].cpu().numpy().transpose(1,2,0)\n",
        "\n",
        "            target_mask_float = np.float32(mask[:,:,target_category]) * full_img_gt\n",
        "            target_Confidence_score = target_mask_float[np.nonzero(target_mask_float)]\n",
        "            target_Confidence_score = np.mean(target_Confidence_score)\n",
        "            print(\"Model Target Confidence Score: \", target_Confidence_score)\n",
        "\n",
        "            logist_softmax_entropy = prob_2_entropy(y_pred)\n",
        "            target_entropy_mask = logist_softmax_entropy[0, :, :, :].detach().cpu().numpy().transpose(1,2,0)\n",
        "            target_entropy_mask_class = target_entropy_mask[:,:,target_category] * full_img_gt\n",
        "            model_entropy = np.mean(target_entropy_mask_class)\n",
        "            print(\"Model Target Entropy Score: \", model_entropy )\n",
        "\n",
        "\n",
        "            targets = [SemanticSegmentationTarget(target_category, target_mask_float)]\n",
        "\n",
        "            nan_condition = np.count_nonzero(target_mask_float)\n",
        "            if nan_condition != 0:\n",
        "\n",
        "                print('running seg-grad-cam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[0], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as cam:\n",
        "                    grayscale_cam_EX = cam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_sgc, heatmap_sgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX, image_weight)\n",
        "                    [confidence_sgc_M2, entropy_sgc_M2, bw_sgc,E_sgc] = XAI_EVAL_M2(0.4,grayscale_cam_EX, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC Target Confidence Score: \", confidence_sgc_M2,\" Entropy Score: \", entropy_sgc_M2)\n",
        "\n",
        "                print('running GradCam++ ...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[3], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as campp:\n",
        "                    grayscale_cam_EX_Plusplus = campp(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_sgcpp, heatmap_sgcpp_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Plusplus, image_weight)\n",
        "                    [confidence_sgcpp_M2, entropy_sgcpp_M2, bw_sgcpp,E_sgcpp] = XAI_EVAL_M2(0.4,grayscale_cam_EX_Plusplus, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-PP Target Confidence Score: \", confidence_sgcpp_M2,\" Entropy Score: \", entropy_sgcpp_M2)\n",
        "\n",
        "                print('running XGradCam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[4], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as xcam:\n",
        "                    grayscale_cam_EX_X = xcam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_Xsgc, heatmap_Xsgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_X, image_weight)\n",
        "                    [confidence_Xsgc_M2, entropy_Xsgc_M2, bw_sgcx,E_sgcx] = XAI_EVAL_M2(0.4,grayscale_cam_EX_X, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-X Target Confidence Score: \", confidence_Xsgc_M2,\" Entropy Score: \", entropy_Xsgc_M2)\n",
        "\n",
        "\n",
        "                print('running ScoreCam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[5], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as scorecam:\n",
        "                    grayscale_cam_EX_Score = scorecam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_scoresgc, heatmap_scoresgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_Score, image_weight)\n",
        "                    [confidence_ssgc_M2, entropy_ssgc_M2, bw_sgcs,E_sgcs] = XAI_EVAL_M2(0.4,grayscale_cam_EX_Score, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-Score Target Confidence Score: \", confidence_ssgc_M2,\" Entropy Score: \", entropy_ssgc_M2)\n",
        "\n",
        "\n",
        "                print('running EigenCam...')\n",
        "                with GRADCAM_Extensions(extension = XAI_method[7], model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as ecam:\n",
        "                    grayscale_cam_EX_eigen = ecam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_eigensgc, heatmap_eigensgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_eigen, image_weight)\n",
        "                    [confidence_esgc_M2, entropy_esgc_M2, bw_sgce, E_sgce] = XAI_EVAL_M2(0.4,grayscale_cam_EX_eigen, full_img_gt, full_img, model, rrp_info,target_category)\n",
        "                    print(\"SGC-eigen Target Confidence Score: \", confidence_esgc_M2,\" Entropy Score: \", entropy_esgc_M2)\n",
        "\n",
        "\n",
        "\n",
        "                print('running AblationCAM...')\n",
        "                with GRADCAMEXTENDED_AblationCAM(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available()) as ablcam:\n",
        "                    grayscale_cam_EX_abl = ablcam(input_tensor=x, targets=targets)[0, :]\n",
        "                    [Exmap_ablsgc, heatmap_ablsgc_rgba] = show_cam_image_whu(full_img_rgba, grayscale_cam_EX_abl, image_weight)\n",
        "                    [confidence_asgc_M2, entropy_asgc_M2, bw_sgca, E_sgca] = XAI_EVAL_M2(0.4,grayscale_cam_EX_abl, full_img_gt, full_img, model, rrp_info, target_category)\n",
        "                    print(\"SGC-Ablbation Target Confidence Score: \", confidence_asgc_M2,\" Entropy Score: \", entropy_asgc_M2)\n",
        "\n",
        "\n",
        "\n",
        "                Model_Seg_Score[counter] = target_Confidence_score\n",
        "                Model_Seg_Entropy[counter] = model_entropy\n",
        "\n",
        "                Seg_Score[0,counter] = confidence_sgc_M2\n",
        "                Seg_Score[1,counter] = confidence_sgcpp_M2\n",
        "                Seg_Score[2,counter] = confidence_Xsgc_M2\n",
        "                Seg_Score[3,counter] = confidence_ssgc_M2\n",
        "                Seg_Score[4,counter] = confidence_esgc_M2\n",
        "                Seg_Score[5,counter] = confidence_asgc_M2\n",
        "\n",
        "                Seg_Entropy[0,counter] = entropy_sgc_M2\n",
        "                Seg_Entropy[1,counter] = entropy_sgcpp_M2\n",
        "                Seg_Entropy[2,counter] = entropy_Xsgc_M2\n",
        "                Seg_Entropy[3,counter] = entropy_ssgc_M2\n",
        "                Seg_Entropy[4,counter] = entropy_esgc_M2\n",
        "                Seg_Entropy[5,counter] = entropy_asgc_M2\n",
        "                counter = counter + 1\n",
        "\n",
        "                if(number_testing_images == 2):\n",
        "                    break\n",
        "\n",
        "\n",
        "\n",
        "                fig = plt.figure(figsize=(8 * scale_fig, 8 * scale_fig))\n",
        "\n",
        "                plt.subplot(3,3,1)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(full_img_rgba)\n",
        "                plt.title('Input image', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,2)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(full_img_gt, cmap='gray', vmin=0, vmax=1)\n",
        "                plt.title('Ground Truth (GT) Mask', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,3)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(target_mask_float,  cmap='gray', vmin=0, vmax=1)\n",
        "                plt.title('Predicted Mask', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,4)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_sgc)\n",
        "                plt.title('Seg-Grad-CAM', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,5)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_sgcpp)\n",
        "                plt.title('Seg-Grad-CAM ++', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,6)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_Xsgc)\n",
        "                plt.title('Seg-XGrad-CAM', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,7)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_scoresgc)\n",
        "                plt.title('Seg-Score-CAM', fontsize=fonts)\n",
        "\n",
        "                plt.subplot(3,3,8)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_eigensgc)\n",
        "                plt.title('Seg-Eigen-CAM', fontsize=fonts)\n",
        "\n",
        "\n",
        "                plt.subplot(3,3,9)\n",
        "                plt.axis('off')\n",
        "                plt.imshow(Exmap_ablsgc)\n",
        "                plt.title('Seg-Ablation-CAM', fontsize=fonts)\n",
        "\n",
        "                # Save the full figure...\n",
        "                fig.savefig('./results/SegGradCam_Extensions_{}_{}.png'.format(filename,decoder_idx), bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAO-zOUAVT_C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65ad4f3d0b0d447eb3ca6b0949607cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6c1e8c98cc24ac5a72ab32d68382729",
              "IPY_MODEL_a73b3088da4a4a8ab88aa9c3ee14c704",
              "IPY_MODEL_4caa6d810e1849d5a43454cedf5708b3"
            ],
            "layout": "IPY_MODEL_bc90a504b46e4c5e88e6a0f872e81447"
          }
        },
        "e6c1e8c98cc24ac5a72ab32d68382729": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68142e32f3d14621abc72fba6203cecd",
            "placeholder": "​",
            "style": "IPY_MODEL_78a8808712554975911c5ffdea9c5378",
            "value": "config.json: 100%"
          }
        },
        "a73b3088da4a4a8ab88aa9c3ee14c704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f4ea7c5a36248b5ba0b4d26e81d1823",
            "max": 156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3636534f29614412af6c2bcd0fd92905",
            "value": 156
          }
        },
        "4caa6d810e1849d5a43454cedf5708b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d014f1c2a84643a089930c0d20699e2d",
            "placeholder": "​",
            "style": "IPY_MODEL_0be65b8285a04fcda6d4f3ca72d893e3",
            "value": " 156/156 [00:00&lt;00:00, 8.33kB/s]"
          }
        },
        "bc90a504b46e4c5e88e6a0f872e81447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68142e32f3d14621abc72fba6203cecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a8808712554975911c5ffdea9c5378": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f4ea7c5a36248b5ba0b4d26e81d1823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3636534f29614412af6c2bcd0fd92905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d014f1c2a84643a089930c0d20699e2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0be65b8285a04fcda6d4f3ca72d893e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcec344add5b4186bd8310dc02b86861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e5a98b54833c4428b6c81fa2033eed39",
              "IPY_MODEL_22dab3ccf07843848e8285def6dad267",
              "IPY_MODEL_372fa7d07eca43b5b06b90432279dd2e"
            ],
            "layout": "IPY_MODEL_ef9c00cba40b44dbba8cd0975e42c05e"
          }
        },
        "e5a98b54833c4428b6c81fa2033eed39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a26e813fc714a8c9405836ae39e102d",
            "placeholder": "​",
            "style": "IPY_MODEL_f93c43ff45ae44bdb96267a91bd045fd",
            "value": "model.safetensors: 100%"
          }
        },
        "22dab3ccf07843848e8285def6dad267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28fe152d928748cebbf510fc6b7a9fc5",
            "max": 87275112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_edb8eb1b563a4635afa306fd5a17c05b",
            "value": 87275112
          }
        },
        "372fa7d07eca43b5b06b90432279dd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b085bbe5e4ee4a3184fb9fcafd0bb1e2",
            "placeholder": "​",
            "style": "IPY_MODEL_3013d65f860f4da3baa221e984b9c9df",
            "value": " 87.3M/87.3M [00:00&lt;00:00, 399MB/s]"
          }
        },
        "ef9c00cba40b44dbba8cd0975e42c05e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a26e813fc714a8c9405836ae39e102d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f93c43ff45ae44bdb96267a91bd045fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28fe152d928748cebbf510fc6b7a9fc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edb8eb1b563a4635afa306fd5a17c05b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b085bbe5e4ee4a3184fb9fcafd0bb1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3013d65f860f4da3baa221e984b9c9df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}