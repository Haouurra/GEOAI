{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geoaigroup/Aerial-SAM/blob/main/AerialSAM_GEOAI_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFsSKaOtWY2d"
      },
      "source": [
        "Samples used in this demo is from the WHU Building Dataset\n",
        "\n",
        "WHU dataset : https://paperswithcode.com/dataset/whu-building-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1ae39ff"
      },
      "source": [
        "# Object masks from prompts with SAM\n",
        "This section was prepared by Ali Mayladan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a4b25c"
      },
      "source": [
        "The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.\n",
        "\n",
        "The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644532a8"
      },
      "source": [
        "## Environment Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07fabfee"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXi87ep3DNxC",
        "outputId": "5144268e-5fe5-4b46-e7b4-442acbdfcb48"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/geoaigroup/Aerial-SAM/raw/main/resources/data.zip\n",
        "!wget https://github.com/geoaigroup/Aerial-SAM/raw/main/resources/pred_shapefile.zip\n",
        "!unzip data.zip\n",
        "!unzip pred_shapefile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ea65efc"
      },
      "outputs": [],
      "source": [
        "using_colab = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91dd9a89"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(\"PyTorch version:\", torch.__version__)\n",
        "    print(\"Torchvision version:\", torchvision.__version__)\n",
        "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "    import sys\n",
        "    !{sys.executable} -m pip install opencv-python matplotlib\n",
        "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "    !pip install geopandas\n",
        "    !pip install rasterio\n",
        "    !git clone https://github.com/geoaigroup/buildingsSAM.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be845da"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33681dd1"
      },
      "source": [
        "Necessary imports and helper functions for displaying points, boxes, and masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69b28288"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import geopandas as gpd\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import shapely.geometry as sg\n",
        "from shapely import affinity\n",
        "from shapely.geometry import Point, Polygon\n",
        "import random\n",
        "from PIL import Image, ImageDraw\n",
        "import rasterio\n",
        "from rasterio.features import geometry_mask\n",
        "#from metrics import DiceScore,IoUScore\n",
        "import pandas as pd\n",
        "import gc\n",
        "import shutil\n",
        "import fiona\n",
        "import json\n",
        "from buildingsSAM import utils\n",
        "from buildingsSAM.evaluate import cal_scores\n",
        "from buildingsSAM.pred_SAM import SAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTMYJz817o2c"
      },
      "outputs": [],
      "source": [
        "def main(CNN=\"\",prompt_type=\"\",sam=None):\n",
        "    score_list = []\n",
        "    # ff = gpd.read_file(pred)\n",
        "    ids = [f for f in os.listdir(orig_shp)]\n",
        "    for name in tqdm(ids):\n",
        "        print(name)\n",
        "        print(\"Checking\")\n",
        "        flag=0\n",
        "        if glob.glob(output_dir + \"/\" + name + \"/\" + name + \".shp\" ) or glob.glob(output_dir + \"/\" + name + \"/\" + name + \".png\" ):\n",
        "            print(\"Found\")\n",
        "            continue\n",
        "\n",
        "        tile_boxes = []\n",
        "        try:\n",
        "            image = cv2.imread(images + \"/\" + name+'.png')\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(name)\n",
        "\n",
        "        if CNN==\"multiclassUnet\":\n",
        "\n",
        "            if name in os.listdir(orig_shp):\n",
        "                gt = gpd.read_file(orig_shp + \"/\" + name)\n",
        "                if len(gt[\"geometry\"]) == 0:\n",
        "                    continue\n",
        "            else:\n",
        "                continue\n",
        "            predic = ff.loc[ff[\"ImageId\"] == name]\n",
        "\n",
        "        elif CNN==\"DCNN\":\n",
        "            predic = gpd.read_file(pred+\"/\"+dirr+\"/\"+name)\n",
        "\n",
        "        geo = predic[\"geometry\"]\n",
        "\n",
        "        if len(geo) == 0:\n",
        "            continue\n",
        "\n",
        "        input_point=[]\n",
        "        input_label=[]\n",
        "\n",
        "        ###creating boxes\n",
        "        if prompt_type==\"box\":\n",
        "            tile_boxes=utils.create_boxes(geo)\n",
        "            input_boxes=torch.tensor(tile_boxes).cuda()\n",
        "\n",
        "        ##Skeleton\n",
        "        if prompt_type==\"skeleton\":\n",
        "            with open(skeleton_points, 'r') as json_file:\n",
        "                data = json.load(json_file)\n",
        "            matching_items = []\n",
        "            for item in data:\n",
        "                if item['id'] == name:\n",
        "                    matching_items.append(item)\n",
        "\n",
        "            input_point=torch.Tensor(matching_items[0]['input_points']).cuda()\n",
        "            input_label=torch.Tensor(matching_items[0]['input_labels']).cuda().long()\n",
        "\n",
        "        x = torch.from_numpy(image.transpose(2, 0, 1)).float().cuda()\n",
        "        pred_mask=sam.predictSAM(x=x,image=image,input_point=input_point,input_label=input_label,input_boxes=input_boxes,flag=flag)\n",
        "        os.makedirs(score_dir, exist_ok=True)\n",
        "        os.makedirs(output_dir + \"/\" + f\"{name}\", exist_ok=True)\n",
        "        scores=cal_scores(output_dir,score_dir)\n",
        "        scores.micro_match_iou(pred_mask,name,gt,score_list,image,tile_boxes,geo=geo)\n",
        "    scores.macro_score()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BR_5fc6N9S7"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "images = \"data/images\"\n",
        "output_dir = \"data/output\"\n",
        "pred = \"pred_shapefile\"\n",
        "orig_shp=\"data/orig_shp\"\n",
        "score_dir = \"data/scores\"\n",
        "skeleton_points=\"data/points.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0hmealy7o2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "#loading SAM Model\n",
        "sam=SAM()\n",
        "#load Multiclass Unet CNN prediction file\n",
        "ff = gpd.read_file(pred)\n",
        "\n",
        "#Run SAM prediction with box prompt\n",
        "main(CNN=\"multiclassUnet\",prompt_type=\"box\",sam=sam)\n",
        "\n",
        "#Run SAM prediction with skeleton prompt\n",
        "main(CNN=\"multiclassUnet\",prompt_type=\"skeleton\",sam=sam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X3LU3_oN9S8"
      },
      "outputs": [],
      "source": [
        "#loading SAM Model\n",
        "sam=SAM()\n",
        "pred = \"data/DCNN_pred_shapefile\"\n",
        "main(CNN=\"DCNN\",prompt_type=\"box\",sam=sam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oow_J4wm_qBk"
      },
      "source": [
        "LangSAM : https://github.com/luca-medeiros/lang-segment-anything\n",
        "\n",
        "GroundingDINO: https://github.com/IDEA-Research/GroundingDINO\n",
        "\n",
        "Segment Anything : https://github.com/facebookresearch/segment-anything\n",
        "\n",
        "**Language Segment-Anything (LangSAM)** is an open-source project that combines the power of instance segmentation and text prompts to generate masks for specific objects in images. Built on the recently released Meta model, segment-anything, and the GroundingDINO detection model, it's an easy-to-use and effective tool for object detection and image segmentation.\n",
        "\n",
        "This section was prepared by Hasan Moughnieh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyCDYhkDEWCO"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/geoaigroup/geoaigroup-website/raw/main/content/media/SAM_26May2023/LangSAM.zip\n",
        "!unzip LangSAM.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAtDoF_-_4uR"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install opencv-python matplotlib\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bia-pl5_61v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rYB8y40_8dh"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!pip install -q -e .\n",
        "!pip install -q roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB58CI0QWDui"
      },
      "outputs": [],
      "source": [
        "!python pre_langsam.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrstnu_fARcY"
      },
      "source": [
        "\n",
        "\n",
        "box_threshold: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.\n",
        "\n",
        "text_threshold: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.\n",
        "\n",
        "The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2e5pW-SAPHD"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from LangSAM import LangSAM\n",
        "from demo_functions import load_ground_truth_masks , display_images_with_masks\n",
        "\n",
        "index = '2_34'\n",
        "#replace 2_36 and 2_37 for more samples\n",
        "image = f'/content/{index}_img.png'\n",
        "ground_truth_mask = f'/content/{index}_gt.png'\n",
        "\n",
        "model = LangSAM()\n",
        "\n",
        "image_pil = Image.open(image).convert(\"RGB\")\n",
        "text_prompt = \"house\"\n",
        "masks, boxes, phrases, logits = model.predict(image_pil, text_prompt)\n",
        "ground_truth_masks = load_ground_truth_masks(ground_truth_mask)\n",
        "\n",
        "#This function displays the original image , predicted masks , and accuracy compared to ground truth\n",
        "display_images_with_masks(image_pil, masks , boxes, ground_truth_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSPcAiI13Kix"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmG9O1WM_Jaz"
      },
      "outputs": [],
      "source": [
        "from demo_functions import MaskMatchingAlgorithm\n",
        "\n",
        "matcher = MaskMatchingAlgorithm(ground_truth_mask, masks)\n",
        "output = matcher.matching()\n",
        "iou_list = output[0]\n",
        "tp_pred_indices = output[1]\n",
        "tp_gt_indices = output[2]\n",
        "fp_indices = output[3]\n",
        "fn_indices = output[4]\n",
        "tp_iou_list, avg_tp_iou = matcher.tp_iou(tp_pred_indices, tp_gt_indices)\n",
        "average_iou, avg_tp_iou , precision , recall , f1_score, tp_f1 = matcher.display_results(iou_list ,tp_pred_indices,\n",
        "                                                                                                tp_gt_indices,\n",
        "                                                                                                fp_indices,\n",
        "                                                                                                fn_indices ,\n",
        "                                                                                                tp_iou_list,\n",
        "                                                                                                avg_tp_iou)\n",
        "print('average IoU:' , average_iou)\n",
        "print(\"average TP IoU\" , avg_tp_iou)\n",
        "print(\"precision\" , precision)\n",
        "print(\"recall\" , recall)\n",
        "print(\"F1 score\" , f1_score)\n",
        "print(\"TP F1 score\" , tp_f1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
